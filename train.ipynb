{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import namedtuple\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "\n",
    "class VGG16(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False):\n",
    "        super(VGG16, self).__init__()\n",
    "        vgg_pretrained_features = models.vgg16(pretrained=True).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\"])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            ConvBlock(3, 32, kernel_size=9, stride=1),\n",
    "            ConvBlock(32, 64, kernel_size=3, stride=2),\n",
    "            ConvBlock(64, 128, kernel_size=3, stride=2),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ConvBlock(128, 64, kernel_size=3, upsample=True),\n",
    "            ConvBlock(64, 32, kernel_size=3, upsample=True),\n",
    "            ConvBlock(32, 3, kernel_size=9, stride=1, normalize=False, relu=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=True),\n",
    "            ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x) + x\n",
    "\n",
    "\n",
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, upsample=False, normalize=True, relu=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(kernel_size // 2), nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "        )\n",
    "        self.norm = nn.InstanceNorm2d(out_channels, affine=True) if normalize else None\n",
    "        self.relu = relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            x = F.interpolate(x, scale_factor=2)\n",
    "        x = self.block(x)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        if self.relu:\n",
    "            x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize((256,256), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "]\n",
    "transform = transforms.Compose(transforms_)\n",
    "class dataload(Dataset):\n",
    "    def __init__(self,train:bool):\n",
    "        self.train=train\n",
    "        self.raw_path='/home/govind/Govind/UWCNN/data/type_3/type3_data/gt_typeI'\n",
    "    def __len__(self):\n",
    "        if (self.train):\n",
    "            return 1440\n",
    "        \n",
    "        else:\n",
    "            return 49\n",
    "    def __getitem__(self, index):\n",
    "        if (self.train):\n",
    "            st_idx=0\n",
    "        else:\n",
    "            st_idx=800\n",
    "        raw_files=os.listdir(self.raw_path)\n",
    "        raw_image=Image.open(os.path.join(self.raw_path,raw_files[st_idx+index]))\n",
    "        raw_image=transform(raw_image)\n",
    "        # raw_image=torch.tensor(np.array(raw_image)).permute(2,1,0).float()\n",
    "        # gt_image=torch.tensor(np.array(gt_image)).permute(2,1,0).float()\n",
    "        return raw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataload(True)\n",
    "dataloader = DataLoader(train_dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer = TransformerNet().to('cuda')\n",
    "vgg = VGG16(requires_grad=False).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), 1e-3)\n",
    "l2_loss = torch.nn.MSELoss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transform(image_size=None):\n",
    "    \"\"\" Transforms for style image \"\"\"\n",
    "    resize = [transforms.Resize(image_size)] if image_size else []\n",
    "    transform = transforms.Compose(resize + [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = style_transform(256)(Image.open('/home/govind/Govind/Bash/code/Fast-Neural-Style-Transfer/images/styles/style_image.jpg'))\n",
    "style = style.repeat(1, 1, 1, 1).to(device)\n",
    "#/home/govind/dashot/neural-style-transfer-pytorch/images/style/2.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def gram_matrix(y):\n",
    "    \"\"\" Returns the gram matrix of y (used to compute style loss) \"\"\"\n",
    "    (b, c, h, w) = y.size()\n",
    "    features = y.view(b, c, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    gram = features.bmm(features_t) / (c * h * w)\n",
    "    return gram\n",
    "\n",
    "\n",
    "\n",
    "def train_transform(image_size):\n",
    "    \"\"\" Transforms for training images \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(int(image_size * 1.15)),\n",
    "            transforms.RandomCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "\n",
    "def denormalize(tensors):\n",
    "    \"\"\" Denormalizes image tensors using mean and std \"\"\"\n",
    "    for c in range(3):\n",
    "        tensors[:, c].mul_(std[c]).add_(mean[c])\n",
    "    return tensors\n",
    "\n",
    "\n",
    "\n",
    "def deprocess(image_tensor):\n",
    "    \"\"\" Denormalizes and rescales image tensor \"\"\"\n",
    "    image_tensor = denormalize(image_tensor)[0]\n",
    "    image_tensor *= 255\n",
    "    image_np = torch.clamp(image_tensor, 0, 255).cpu().numpy().astype(np.uint8)\n",
    "    image_np = image_np.transpose(1, 2, 0)\n",
    "    return image_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_style = vgg(style)\n",
    "gram_style = [gram_matrix(y) for y in features_style]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/20] [Batch 1439/1440] [Content: 768070.12 (808564.84) Style: 457461.69 (625900.84) Total: 1225531.75 (1434465.69)]]"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "        epoch_metrics = {\"content\": [], \"style\": [], \"total\": []}\n",
    "        for batch_i, (images) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            images_original = images.to(device)\n",
    "            images_transformed = transformer(images_original)\n",
    "\n",
    "            # Extract features\n",
    "            features_original = vgg(images_original)\n",
    "            features_transformed = vgg(images_transformed)\n",
    "\n",
    "            # Compute content loss as MSE between features\n",
    "            content_loss = 1e5 * l2_loss(features_transformed.relu2_2, features_original.relu2_2)\n",
    "\n",
    "            # Compute style loss as MSE between gram matrices\n",
    "            style_loss = 0\n",
    "            for ft_y, gm_s in zip(features_transformed, gram_style):\n",
    "                gm_y = gram_matrix(ft_y)\n",
    "                style_loss += l2_loss(gm_y, gm_s[: images.size(0), :, :])\n",
    "            style_loss *= 1e10\n",
    "\n",
    "            total_loss = content_loss + style_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_metrics[\"content\"] += [content_loss.item()]\n",
    "            epoch_metrics[\"style\"] += [style_loss.item()]\n",
    "            epoch_metrics[\"total\"] += [total_loss.item()]\n",
    "\n",
    "            sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] [Content: %.2f (%.2f) Style: %.2f (%.2f) Total: %.2f (%.2f)]\"\n",
    "                % (\n",
    "                    epoch + 1,\n",
    "                    20,\n",
    "                    batch_i,\n",
    "                    len(train_dataset),\n",
    "                    content_loss.item(),\n",
    "                    np.mean(epoch_metrics[\"content\"]),\n",
    "                    style_loss.item(),\n",
    "                    np.mean(epoch_metrics[\"style\"]),\n",
    "                    total_loss.item(),\n",
    "                    np.mean(epoch_metrics[\"total\"]),\n",
    "                )\n",
    "            )\n",
    "        torch.save(transformer.state_dict(), f\"model/{epoch}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "govindenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
